"""
TinyLlama ëª¨ë¸ì„ PPO(Proximal Policy Optimization)ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
ë³´ìƒ í•¨ìˆ˜: ê¸ì •ì ì¸ ê°ì • ë‹¨ì–´ë¥¼ ìƒì„±í•˜ë©´ ë³´ìƒì„ ë°›ìŠµë‹ˆë‹¤.
"""

import torch
from transformers import AutoTokenizer
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
from peft import LoraConfig
import os

# ==========================================
# 1. ì„¤ì • (Configuration)
# ==========================================

# ëª¨ë¸ ì„¤ì •
MODEL_ID = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
OUTPUT_DIR = "./trained_model"

# LoRA ì„¤ì • (ê±°ëŒ€ ëª¨ë¸ ì „ì²´ê°€ ì•„ë‹ˆë¼, ì´ ë¶€ë¶„ë§Œ í•™ìŠµí•©ë‹ˆë‹¤)
LORA_CONFIG = LoraConfig(
    r=16,               # LoRA ë­í¬
    lora_alpha=32,      # í•™ìŠµ ê°•ë„
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

# PPO ì„¤ì • (í•™ìŠµë¥  UP, ë³µìŠµ UP)
PPO_CONFIG = PPOConfig(
    learning_rate=3e-5,  # ê°œì„ ëœ í•™ìŠµë¥  (2e-5 -> 3e-5)
    batch_size=10,
    mini_batch_size=2,
    gradient_accumulation_steps=5,
    ppo_epochs=4,        # ê°™ì€ ë°ì´í„°ë¡œ 4ë²ˆ ë°˜ë³µ í•™ìŠµ (ë³µìŠµ)
    init_kl_coef=0.2,    # KL í˜ë„í‹° ì‹œì‘ê°’ (ê¸°ë³¸ê°’ 0.1ë³´ë‹¤ 2ë°° ë†’ì„)
    adap_kl_ctrl=True,   # ìƒí™©ì— ë§ì¶°ì„œ í˜ë„í‹° ê°•ë„ë¥¼ AIê°€ ìë™ ì¡°ì ˆ
)

# í•™ìŠµ ì„¤ì •
TRAINING_EPOCHS = 150  # ê°œì„ ëœ ì—í¬í¬ ìˆ˜ (100 -> 150)
TARGET_BATCH_SIZE = 10
# í™•ì¥ëœ ì…ë ¥ ì¿¼ë¦¬ (ë” ë‹¤ì–‘í•œ ë¬¸ì¥ íŒ¨í„´)
QUERIES = [
    "I feel so",
    "This makes me",
    "I am really",
    "Today I am",
    "Life is",
    "I'm feeling"
]
# í™•ì¥ëœ ê¸ì • ë‹¨ì–´ ëª©ë¡ (ë” ë‹¤ì–‘í•œ ê°ì • í‘œí˜„)
TARGET_WORDS = [
    "happy", "glad", "good", "great", "smile", "joy", "love",
    "wonderful", "amazing", "excited", "pleased", "delighted",
    "cheerful", "grateful", "blessed", "fantastic"
]

# ==========================================
# 2. ë””ë°”ì´ìŠ¤ ì„¤ì •
# ==========================================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
if device == "cpu":
    print("âš ï¸  ê²½ê³ : CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•˜ë©´ ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# ==========================================
# 3. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì¤€ë¹„
# ==========================================
print(f"\nëª¨ë¸({MODEL_ID})ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")

try:
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)

    # TinyLlamaëŠ” íŒ¨ë”© í† í°ì´ ë”°ë¡œ ì—†ì–´ì„œ ì„¤ì •í•´ì¤˜ì•¼ í•¨
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # ëª¨ë¸ ë¡œë“œ (LoRA ì ìš©)
    model = AutoModelForCausalLMWithValueHead.from_pretrained(
        MODEL_ID,
        peft_config=LORA_CONFIG,
        device_map="auto" if device == "cuda" else None
    )

    # PPO íŠ¸ë ˆì´ë„ˆ ìƒì„±
    ppo_trainer = PPOTrainer(
        config=PPO_CONFIG,
        model=model,
        tokenizer=tokenizer,
    )

    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    raise

# ==========================================
# 4. ë³´ìƒ í•¨ìˆ˜ (Reward Function)
# ==========================================
def get_reward(generated_text):
    """
    ìƒì„±ëœ í…ìŠ¤íŠ¸ì— ê¸ì •ì ì¸ ê°ì • ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ë³´ìƒì„ ì¤ë‹ˆë‹¤.

    Args:
        generated_text (str): ìƒì„±ëœ í…ìŠ¤íŠ¸

    Returns:
        float: ë³´ìƒ ê°’ (1.0 ë˜ëŠ” -1.0)
    """
    # í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ì„±ê³µ!
    for word in TARGET_WORDS:
        if word in generated_text.lower():
            return 1.0

    return -1.0

# ==========================================
# 5. í•™ìŠµ ë£¨í”„ (Training Loop)
# ==========================================
print("\n=== í›ˆë ¨ ì‹œì‘ ===")
print(f"í›ˆë ¨ ì—í¬í¬: {TRAINING_EPOCHS}")
print(f"ë°°ì¹˜ í¬ê¸°: {TARGET_BATCH_SIZE}")
print(f"ëª©í‘œ ë‹¨ì–´: {', '.join(TARGET_WORDS)}\n")

batch_queries = []
batch_responses = []
batch_rewards = []

generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 10
}

total_steps = 0
positive_generations = 0

try:
    for epoch in range(TRAINING_EPOCHS):
        for query_txt in QUERIES:
            # ì…ë ¥ ì¤€ë¹„
            inputs = tokenizer(query_txt, return_tensors="pt")
            query_tensors = inputs.input_ids.to(device)

            # í…ìŠ¤íŠ¸ ìƒì„±
            response_tensors = ppo_trainer.generate(query_tensors[0], **generation_kwargs)
            response_txt = tokenizer.decode(response_tensors[0], skip_special_tokens=True)
            generated_part = response_txt[len(query_txt):]

            # ë³´ìƒ ê³„ì‚°
            reward_value = get_reward(generated_part)

            # ë°°ì¹˜ì— ì¶”ê°€
            batch_queries.append(query_tensors[0])
            batch_responses.append(response_tensors[0])
            batch_rewards.append(torch.tensor(reward_value).to(device))

            # ë°°ì¹˜ê°€ ì°¼ìœ¼ë©´ í•™ìŠµ ìˆ˜í–‰
            if len(batch_queries) == TARGET_BATCH_SIZE:
                stats = ppo_trainer.step(batch_queries, batch_responses, batch_rewards)
                total_steps += 1

                # ë°°ì¹˜ ì´ˆê¸°í™”
                batch_queries = []
                batch_responses = []
                batch_rewards = []

                print(f"ğŸ”¥ [Epoch {epoch+1}/{TRAINING_EPOCHS}] Step {total_steps} í•™ìŠµ ì™„ë£Œ")

            # ê¸ì •ì ì¸ ìƒì„± ê²°ê³¼ í‘œì‹œ
            if reward_value > 0:
                positive_generations += 1
                print(f"ğŸ‰ [Epoch {epoch+1}] ë°œê²¬! '{generated_part.strip()}'")

        # 10 ì—í¬í¬ë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
        if (epoch + 1) % 10 == 0:
            success_rate = (positive_generations / ((epoch + 1) * len(QUERIES))) * 100
            print(f"\nğŸ“Š [ì§„í–‰ ìƒí™©] Epoch {epoch+1}/{TRAINING_EPOCHS}")
            print(f"   ê¸ì • ìƒì„± íšŸìˆ˜: {positive_generations}")
            print(f"   ì„±ê³µë¥ : {success_rate:.1f}%\n")

except KeyboardInterrupt:
    print("\n\nâš ï¸  í›ˆë ¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"\n\nâŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    raise

print("\n=== í›ˆë ¨ ì¢…ë£Œ ===")
print(f"ì´ í•™ìŠµ ìŠ¤í…: {total_steps}")
print(f"ê¸ì • ìƒì„± íšŸìˆ˜: {positive_generations}")
print(f"ìµœì¢… ì„±ê³µë¥ : {(positive_generations / (TRAINING_EPOCHS * len(QUERIES))) * 100:.1f}%")

# ==========================================
# 6. ëª¨ë¸ ì €ì¥
# ==========================================
print(f"\nëª¨ë¸ì„ ì €ì¥í•˜ëŠ” ì¤‘... ({OUTPUT_DIR})")
try:
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"âœ… ëª¨ë¸ì´ {OUTPUT_DIR}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
except Exception as e:
    print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

# ==========================================
# 7. í…ŒìŠ¤íŠ¸ ìƒì„±
# ==========================================
print("\n=== í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
test_queries = ["I feel so", "This makes me", "Today I am"]

for test_query in test_queries:
    inputs = tokenizer(test_query, return_tensors="pt").to(device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=15,
        do_sample=True,
        top_p=0.9,
        temperature=0.8,
        pad_token_id=tokenizer.eos_token_id
    )
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"ì…ë ¥: '{test_query}'")
    print(f"ìƒì„±: '{generated_text}'\n")

print("ì™„ë£Œ! ğŸ‰")
