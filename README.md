# TinyLlama RLHF Training Project

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kaz264/tinyllama-rlhf-training/blob/master/tinyllama_rlhf_colab.ipynb)

PPO(Proximal Policy Optimization)ë¥¼ ì‚¬ìš©í•œ TinyLlama ëª¨ë¸ ê°•í™”í•™ìŠµ íŒŒì¸íŠœë‹ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

**ğŸš€ ë¹ ë¥¸ ì‹œì‘**: ìœ„ì˜ "Open in Colab" ë°°ì§€ë¥¼ í´ë¦­í•˜ì—¬ Google Colabì—ì„œ ë°”ë¡œ ì‹¤í–‰í•´ë³´ì„¸ìš”!

## í”„ë¡œì íŠ¸ ì†Œê°œ

ì´ í”„ë¡œì íŠ¸ëŠ” TinyLlama-1.1B-Chat ëª¨ë¸ì„ RLHF(Reinforcement Learning from Human Feedback) ê¸°ë²•ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ì—¬, ê¸ì •ì ì¸ ê°ì • í‘œí˜„ì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµì‹œí‚µë‹ˆë‹¤.

## ì£¼ìš” íŠ¹ì§•

- **ëª¨ë¸**: TinyLlama/TinyLlama-1.1B-Chat-v1.0 (1.1B íŒŒë¼ë¯¸í„°)
- **í•™ìŠµ ë°©ë²•**: PPO (Proximal Policy Optimization)
- **íš¨ìœ¨ì  í•™ìŠµ**: LoRA (Low-Rank Adaptation) ì ìš©
- **ë³´ìƒ í•¨ìˆ˜**: ê¸ì •ì ì¸ ê°ì • ë‹¨ì–´ ìƒì„± ì‹œ ë³´ìƒ
- **ëª©í‘œ ë‹¨ì–´**: happy, glad, good, great, smile, joy, love

## ê¸°ìˆ  ìŠ¤íƒ

- **PyTorch**: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- **Transformers**: Hugging Face íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **TRL**: Transformer Reinforcement Learning ë¼ì´ë¸ŒëŸ¬ë¦¬
- **PEFT**: Parameter-Efficient Fine-Tuning (LoRA êµ¬í˜„)
- **Accelerate**: ë¶„ì‚° í•™ìŠµ ë° ìµœì í™”
- **bitsandbytes**: ëª¨ë¸ ì–‘ìí™” ë° ìµœì í™”

## ì„¤ì¹˜ ë°©ë²•

### 1. Python í™˜ê²½ ì„¤ì •

Python 3.8 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

### 2. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
pip install -r requirements.txt
```

ë˜ëŠ” ê°œë³„ ì„¤ì¹˜:

```bash
pip install torch transformers==4.40.0 trl==0.8.6 peft==0.10.0 accelerate==0.29.3 bitsandbytes
```

### 3. CUDA ì„¤ì • (ì„ íƒ ì‚¬í•­)

NVIDIA GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, CUDAê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. GPU ì—†ì´ë„ CPUë¡œ ì‹¤í–‰ ê°€ëŠ¥í•˜ì§€ë§Œ, í•™ìŠµ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
python -c "import torch; print(torch.cuda.is_available())"
```

## ì‚¬ìš© ë°©ë²•

### í•™ìŠµ ì‹¤í–‰

```bash
python train_rlhf.py
```

### í•™ìŠµ ê³¼ì •

1. TinyLlama ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
2. LoRA ì„¤ì • ì ìš©
3. PPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
4. 100 ì—í¬í¬ ë™ì•ˆ í•™ìŠµ ìˆ˜í–‰
5. í•™ìŠµëœ ëª¨ë¸ì„ `./trained_model/` ë””ë ‰í† ë¦¬ì— ì €ì¥
6. í…ŒìŠ¤íŠ¸ ë¬¸ì¥ìœ¼ë¡œ ê²°ê³¼ í™•ì¸

### í•™ìŠµ ì¤‘ í‘œì‹œë˜ëŠ” ì •ë³´

- í˜„ì¬ ì—í¬í¬ ë° ìŠ¤í… ë²ˆí˜¸
- ê¸ì •ì ì¸ ë‹¨ì–´ ìƒì„± ë°œê²¬ ì‹œ ì•Œë¦¼
- 10 ì—í¬í¬ë§ˆë‹¤ ì§„í–‰ ìƒí™© ë° ì„±ê³µë¥ 

## í•™ìŠµ ì„¤ì •

### LoRA ì„¤ì •

```python
r=16               # LoRA ë­í¬ (ë‚®ì„ìˆ˜ë¡ íŒŒë¼ë¯¸í„° ì ìŒ)
lora_alpha=32      # í•™ìŠµ ê°•ë„
lora_dropout=0.05  # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
```

### PPO ì„¤ì •

```python
learning_rate=3e-5              # í•™ìŠµë¥  (ê°œì„ ë¨: 2e-5 â†’ 3e-5)
batch_size=10                   # ë°°ì¹˜ í¬ê¸°
mini_batch_size=2               # ë¯¸ë‹ˆ ë°°ì¹˜ í¬ê¸°
gradient_accumulation_steps=5   # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…
ppo_epochs=4                    # PPO ì—í¬í¬ (ë³µìŠµ íšŸìˆ˜)
init_kl_coef=0.2               # KL í˜ë„í‹° ê³„ìˆ˜
```

### í•™ìŠµ íŒŒë¼ë¯¸í„°

- **í›ˆë ¨ ì—í¬í¬**: 150 (ê°œì„ ë¨: 100 â†’ 150)
- **ë°°ì¹˜ í¬ê¸°**: 10
- **ì…ë ¥ ì¿¼ë¦¬**: 6ê°€ì§€ ë‹¤ì–‘í•œ ë¬¸ì¥ íŒ¨í„´
  - "I feel so", "This makes me", "I am really", "Today I am", "Life is", "I'm feeling"
- **ìµœëŒ€ ìƒì„± í† í°**: 10

## ë³´ìƒ í•¨ìˆ˜

ëª¨ë¸ì´ ë‹¤ìŒ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë¥¼ ìƒì„±í•˜ë©´ +1.0ì˜ ë³´ìƒì„ ë°›ìŠµë‹ˆë‹¤:
- **ê¸°ë³¸ ë‹¨ì–´**: happy, glad, good, great, smile, joy, love
- **í™•ì¥ ë‹¨ì–´**: wonderful, amazing, excited, pleased, delighted, cheerful, grateful, blessed, fantastic

ì´ì™¸ì˜ ê²½ìš° -1.0ì˜ í˜ë„í‹°ë¥¼ ë°›ìŠµë‹ˆë‹¤.

## ì¶œë ¥ ê²°ê³¼

### í•™ìŠµ ì¤‘

```
ğŸ”¥ [Epoch 1/150] Step 1 í•™ìŠµ ì™„ë£Œ
ğŸ‰ [Epoch 1] ë°œê²¬! 'happy and excited!'

ğŸ“Š [ì§„í–‰ ìƒí™©] Epoch 10/150
   ê¸ì • ìƒì„± íšŸìˆ˜: 25
   ì„±ê³µë¥ : 55.0%
```

### í•™ìŠµ ì™„ë£Œ í›„

```
=== í›ˆë ¨ ì¢…ë£Œ ===
ì´ í•™ìŠµ ìŠ¤í…: 30
ê¸ì • ìƒì„± íšŸìˆ˜: 120
ìµœì¢… ì„±ê³µë¥ : 40.0%

âœ… ëª¨ë¸ì´ ./trained_model/ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!

=== í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===
ì…ë ¥: 'I feel so'
ìƒì„±: 'I feel so happy and grateful today'
```

## ëª¨ë¸ ì €ì¥

í•™ìŠµì´ ì™„ë£Œë˜ë©´ ëª¨ë¸ì€ `./trained_model/` ë””ë ‰í† ë¦¬ì— ìë™ ì €ì¥ë©ë‹ˆë‹¤.

ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ë°©ë²•:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("./trained_model")
model = AutoModelForCausalLM.from_pretrained("./trained_model")

inputs = tokenizer("I feel so", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0]))
```

## íŒŒì¼ êµ¬ì¡°

```
test_sample_dl/
â”œâ”€â”€ train_rlhf.py         # ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt      # ì˜ì¡´ì„± íŒ¨í‚¤ì§€
â”œâ”€â”€ README.md            # í”„ë¡œì íŠ¸ ë¬¸ì„œ
â””â”€â”€ trained_model/       # í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ (ìë™ ìƒì„±)
```

## ë¬¸ì œ í•´ê²°

### CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±

- ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ì„¸ìš”: `batch_size=5`
- LoRA ë­í¬ë¥¼ ë‚®ì¶”ì„¸ìš”: `r=8`
- ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”

### í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼

- GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
- ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ì„¸ìš”
- ì—í¬í¬ ìˆ˜ë¥¼ ì¤„ì´ì„¸ìš”

### ëª¨ë¸ì´ ê¸ì • ë‹¨ì–´ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŒ

- í•™ìŠµ ì—í¬í¬ë¥¼ ë” ëŠ˜ë¦¬ì„¸ìš”: `TRAINING_EPOCHS = 200`
- í•™ìŠµë¥ ì„ ë” ë†’ì´ì„¸ìš”: `learning_rate=4e-5`
- PPO ì—í¬í¬ë¥¼ ëŠ˜ë¦¬ì„¸ìš”: `ppo_epochs=6`
- ë³´ìƒ ë‹¨ì–´ë¥¼ ë” ì¶”ê°€í•˜ì„¸ìš”

## ì°¸ê³  ìë£Œ

- [TRL Documentation](https://huggingface.co/docs/trl)
- [PEFT Documentation](https://huggingface.co/docs/peft)
- [TinyLlama Model](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
- [PPO Algorithm](https://arxiv.org/abs/1707.06347)

## ë¼ì´ì„¼ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” êµìœ¡ ë° ì—°êµ¬ ëª©ì ìœ¼ë¡œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.

## ê¸°ì—¬

ë²„ê·¸ ë¦¬í¬íŠ¸ë‚˜ ê°œì„  ì œì•ˆì€ ì´ìŠˆë¡œ ë“±ë¡í•´ì£¼ì„¸ìš”!

---

Happy Training! ğŸš€
