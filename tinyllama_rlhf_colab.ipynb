{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# TinyLlama RLHF Training with PPO\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ TinyLlama ëª¨ë¸ì„ PPO(Proximal Policy Optimization)ë¡œ íŒŒì¸íŠœë‹í•©ë‹ˆë‹¤.\n",
    "\n",
    "**Repository**: https://github.com/kaz264/tinyllama-rlhf-training\n",
    "\n",
    "## ì£¼ìš” íŠ¹ì§•\n",
    "- TinyLlama-1.1B-Chat ëª¨ë¸ ì‚¬ìš©\n",
    "- PPO ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê°•í™”í•™ìŠµ\n",
    "- LoRAë¥¼ í†µí•œ íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹\n",
    "- ê¸ì •ì ì¸ ê°ì • í‘œí˜„ ìƒì„± í•™ìŠµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "\n",
    "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. (ì•½ 2-3ë¶„ ì†Œìš”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# GPU í™•ì¸\n",
    "!nvidia-smi\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì¤‘... (2-3ë¶„ ì†Œìš”)\")\n",
    "print(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch>=2.0.0\n",
    "!pip install -q transformers==4.40.0\n",
    "!pip install -q trl==0.8.6\n",
    "!pip install -q peft==0.10.0\n",
    "!pip install -q accelerate==0.29.3\n",
    "!pip install -q bitsandbytes>=0.41.0\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "## 2. GitHub ì €ì¥ì†Œ í´ë¡ \n",
    "\n",
    "í•™ìŠµ ì½”ë“œë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone"
   },
   "outputs": [],
   "source": [
    "# ê¸°ì¡´ ë””ë ‰í† ë¦¬ê°€ ìˆìœ¼ë©´ ì‚­ì œ\n",
    "!rm -rf tinyllama-rlhf-training\n",
    "\n",
    "# ì €ì¥ì†Œ í´ë¡ \n",
    "!git clone https://github.com/kaz264/tinyllama-rlhf-training.git\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ì´ë™\n",
    "%cd tinyllama-rlhf-training\n",
    "\n",
    "print(\"\\nâœ… ì €ì¥ì†Œ í´ë¡  ì™„ë£Œ!\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_files"
   },
   "source": [
    "## 3. íŒŒì¼ í™•ì¸\n",
    "\n",
    "ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ë“¤ì„ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "list_files"
   },
   "outputs": [],
   "source": [
    "print(\"ğŸ“ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼:\")\n",
    "!ls -lh\n",
    "\n",
    "print(\"\\nğŸ“„ train_rlhf.py íŒŒì¼ í™•ì¸:\")\n",
    "!head -20 train_rlhf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_config"
   },
   "source": [
    "## 4. í•™ìŠµ ì„¤ì • í™•ì¸ (ì„ íƒ ì‚¬í•­)\n",
    "\n",
    "í•™ìŠµ ì„¤ì •ì„ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ì£¼ìš” ì„¤ì •ê°’:\n",
    "- **TRAINING_EPOCHS**: 100 (í•™ìŠµ ë°˜ë³µ íšŸìˆ˜)\n",
    "- **TARGET_BATCH_SIZE**: 10 (ë°°ì¹˜ í¬ê¸°)\n",
    "- **learning_rate**: 2e-5 (í•™ìŠµë¥ )\n",
    "- **TARGET_WORDS**: happy, glad, good, great, smile, joy, love\n",
    "\n",
    "Colab ë¬´ë£Œ ë²„ì „ì—ì„œëŠ” ì‹œê°„ ì œí•œì´ ìˆìœ¼ë¯€ë¡œ, ì—í¬í¬ë¥¼ ì¤„ì´ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "modify_config"
   },
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì—í¬í¬ë¥¼ ì¤„ì´ë ¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”\n",
    "# !sed -i 's/TRAINING_EPOCHS = 100/TRAINING_EPOCHS = 30/' train_rlhf.py\n",
    "\n",
    "print(\"í˜„ì¬ í•™ìŠµ ì„¤ì •:\")\n",
    "!grep -E \"TRAINING_EPOCHS|TARGET_BATCH_SIZE|learning_rate|TARGET_WORDS\" train_rlhf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "runtime_check"
   },
   "source": [
    "## 5. GPU ëŸ°íƒ€ì„ í™•ì¸\n",
    "\n",
    "**ì¤‘ìš”**: ì´ ë…¸íŠ¸ë¶ì€ GPU ëŸ°íƒ€ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "GPUê°€ í™œì„±í™”ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´:\n",
    "1. ë©”ë‰´: **ëŸ°íƒ€ì„** â†’ **ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½**\n",
    "2. **í•˜ë“œì›¨ì–´ ê°€ì†ê¸°**ë¥¼ **GPU** (T4 ë˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ GPU)ë¡œ ì„ íƒ\n",
    "3. **ì €ì¥** í´ë¦­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"âœ… GPU ì‚¬ìš© ê°€ëŠ¥!\")\n",
    "    print(f\"   ë””ë°”ì´ìŠ¤: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    print(\"   ëŸ°íƒ€ì„ ìœ í˜•ì„ GPUë¡œ ë³€ê²½í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 6. í•™ìŠµ ì‹œì‘\n",
    "\n",
    "ì´ì œ TinyLlama ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.\n",
    "\n",
    "### ì˜ˆìƒ ì†Œìš” ì‹œê°„:\n",
    "- GPU (T4): ì•½ 30-60ë¶„ (100 ì—í¬í¬ ê¸°ì¤€)\n",
    "- CPU: **ê¶Œì¥í•˜ì§€ ì•ŠìŒ** (ë§¤ìš° ëŠë¦¼)\n",
    "\n",
    "### í•™ìŠµ ì¤‘ í‘œì‹œë˜ëŠ” ì •ë³´:\n",
    "- ëª¨ë¸ ë¡œë”© ì§„í–‰ ìƒí™©\n",
    "- ê° ì—í¬í¬ ë° ìŠ¤í… ì™„ë£Œ ì•Œë¦¼\n",
    "- ê¸ì •ì ì¸ ë‹¨ì–´ ìƒì„± ë°œê²¬ ì‹œ ğŸ‰ ì•Œë¦¼\n",
    "- 10 ì—í¬í¬ë§ˆë‹¤ ì§„í–‰ ìƒí™© ë° ì„±ê³µë¥ \n",
    "\n",
    "**ì°¸ê³ **: ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì¸í•´ ì‹œì‘ì´ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì‹¤í–‰\n",
    "!python train_rlhf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## 7. í•™ìŠµ ê²°ê³¼ í™•ì¸\n",
    "\n",
    "í•™ìŠµì´ ì™„ë£Œë˜ë©´ ìƒì„±ëœ ëª¨ë¸ê³¼ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_results"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\"./trained_model\"):\n",
    "    print(\"âœ… í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\\n\")\n",
    "    print(\"ì €ì¥ëœ íŒŒì¼:\")\n",
    "    !ls -lh ./trained_model/\n",
    "else:\n",
    "    print(\"âš ï¸  ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test"
   },
   "source": [
    "## 8. í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "í•™ìŠµëœ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•´ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_model"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print(\"í•™ìŠµëœ ëª¨ë¸ ë¡œë”© ì¤‘...\\n\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./trained_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./trained_model\", device_map=\"auto\")\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\\n\")\n",
    "print(\"=\"*50)\n",
    "print(\"í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬\n",
    "test_queries = [\n",
    "    \"I feel so\",\n",
    "    \"This makes me\",\n",
    "    \"Today I am\",\n",
    "    \"I am really\",\n",
    "    \"The weather is\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=20,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"ì…ë ¥: '{query}'\")\n",
    "    print(f\"ìƒì„±: '{generated_text}'\\n\")\n",
    "\n",
    "print(\"ì™„ë£Œ! ğŸ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 9. í•™ìŠµëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì„ íƒ ì‚¬í•­)\n",
    "\n",
    "í•™ìŠµëœ ëª¨ë¸ì„ ë¡œì»¬ë¡œ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´ ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_model"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ì„ ì••ì¶•í•˜ì—¬ ë‹¤ìš´ë¡œë“œ\n",
    "!zip -r trained_model.zip ./trained_model/\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë¸ì´ ì••ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"ì™¼ìª½ íŒŒì¼ íƒìƒ‰ê¸°ì—ì„œ 'trained_model.zip'ì„ ì°¾ì•„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.\")\n",
    "\n",
    "# Colabì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ\n",
    "from google.colab import files\n",
    "files.download('trained_model.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### í•™ìŠµ ê°œì„  ë°©ë²•:\n",
    "1. **ì—í¬í¬ ì¦ê°€**: `TRAINING_EPOCHS`ë¥¼ 150-200ìœ¼ë¡œ ëŠ˜ë¦¬ê¸°\n",
    "2. **í•™ìŠµë¥  ì¡°ì •**: `learning_rate`ë¥¼ 3e-5ë¡œ ë†’ì´ê¸°\n",
    "3. **ë³´ìƒ í•¨ìˆ˜ ìˆ˜ì •**: ë” ë‹¤ì–‘í•œ ê¸ì • ë‹¨ì–´ ì¶”ê°€\n",
    "4. **ì¿¼ë¦¬ í™•ì¥**: ë” ë§ì€ ì…ë ¥ ë¬¸ì¥ ì¶”ê°€\n",
    "\n",
    "### ì¶”ê°€ ì‹¤í—˜:\n",
    "- ë‹¤ë¥¸ ê°ì • (ìŠ¬í””, ë¶„ë…¸ ë“±)ìœ¼ë¡œ í•™ìŠµ\n",
    "- ë” í° ëª¨ë¸ ì‚¬ìš© (ì˜ˆ: Llama-2-7B)\n",
    "- DPO (Direct Preference Optimization) ì‹œë„\n",
    "\n",
    "---\n",
    "\n",
    "**ë¬¸ì œê°€ ë°œìƒí•˜ë©´**: [GitHub Issues](https://github.com/kaz264/tinyllama-rlhf-training/issues)ì— ë¬¸ì˜í•˜ì„¸ìš”!\n",
    "\n",
    "Happy Training! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
